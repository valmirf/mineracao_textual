{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05b_Sentiment_Analysis_Imdb.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/mineracao_textual/blob/main/CNN/Exercicio_Analise_Sentimentos_Imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLJozfTGU0Lj"
      },
      "source": [
        "##Análise de Sentimentos\n",
        "A análise de sentimento visa determinar a opinião ou sentimento das pessoas eem relçao a um produto, pessoa, evento, ou qualquer outro objeto de interesse. Por exemplo, um palestrante ou escritor em relação a um documento, interação ou evento. O sentimento é principalmente categorizado em categorias positivas, negativas e neutras. Por meio da análise de sentimento, podemos prever, por exemplo, a opinião e a atitude de um cliente sobre um produto com base em uma resenha que ele escreveu. Essa técnica é amplamente aplicada a coisas como revisões, pesquisas, documentos e muito mais.\n",
        "\n",
        "##Base de Dados IMDB\n",
        "O conjunto de dados de classificação de sentimento do IMDB (https://www.imdb.com/) consiste em 50000 resenhas de filmes de usuários do IMDB que são rotuladas como positivas (1) ou negativas (0). As 50000 resenhas são divididas em 25000 para treinamento e 25000 para teste. O conjunto de dados foi criado por pesquisadores da Universidade de Stanford e publicado em um artigo de 2011, onde alcançou 88.89% de precisão. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIxphGHVdu6w"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "# Load the data, keeping only 10,000 of the most frequently occuring words\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oem7f6Prdu6_"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Since we restricted ourselves to the top 10000 frequent words, no word index should exceed 10000\n",
        "# we'll verify this below\n",
        "# Here is a list of maximum indexes in every review --- we search the maximum index in this list of max indexes\n",
        "print(type([max(sequence) for sequence in train_data]))\n",
        "\n",
        "# Find the maximum of all max indexes\n",
        "max([max(sequence) for sequence in train_data])\n",
        "\n",
        "data = np.concatenate((train_data, test_data), axis=0)\n",
        "labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "\n",
        "print(\"Categories:\", np.unique(labels))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz1pTRryYWtb"
      },
      "source": [
        "Você pode ver acima que o conjunto de dados é classificado em duas categorias, 0 ou 1, que representa o sentimento da avaliação do filme, negativo e positivo respectivamente. Todo o conjunto de dados contém 9.998 palavras únicas e o comprimento médio da revisão é de 234 palavras, com um desvio padrão de 173 palavras. \n",
        "\n",
        "##Decodificação da Avaliação\n",
        "Abaixo, vamos decodificar o rótulo no formato *One-Hot Code*.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX33fS3rLltV"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(labels.reshape(-1, 1))\n",
        "print(y[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHeqvDdtaT8J"
      },
      "source": [
        "##Preparação dos Dados\n",
        "\n",
        "Nos códigos abaixo iremos realizar a separação dos dados em treinamento e teste utilizando a função `train_test_split` do sklearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOx2UvKtaynK"
      },
      "source": [
        "sentences_train, sentences_test, y_train, y_test  = train_test_split(data, y, test_size=0.3, random_state=42, stratify=y)\n",
        "print(sentences_train[:2], y_train[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClM3xhZffWMC"
      },
      "source": [
        "## Sequence Padding\n",
        "\n",
        "Um problema que temos é que cada sequência de texto tem na maioria dos casos diferentes comprimentos de palavras. Para corrigir isso, vamos usar `pad_sequence()` que simplesmente preenche a sequência de palavras com zeros. O número máximo 250 foi escolhido por uma aproximação da média de palavras na avaliação que é 234. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgdxOi8ja7tb"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 250\n",
        "X_train = pad_sequences(sentences_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(sentences_test, padding='post', maxlen=maxlen)\n",
        "print(X_train[100])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZso-lvrdhHW"
      },
      "source": [
        "##Construindo a Rede Neural\n",
        "\n",
        "Agora podemos construir uma rede neural simples. Começaremos definindo o tipo de modelo que queremos construir. Existem dois tipos de modelos disponíveis no Keras: o modelo sequencial e a classe do modelo usado na API funcional.\n",
        "\n",
        "Começamos adicionando a primeira camada padrão ao lidar com Texto e CNNs: a camada de Embeddings. Nesse exercício vamos usar um Embedding de tamanho 300. Depois, camadas de Convolução e MaxPooling são adicionadas e por fim, camadas de classificação Densas, sendo que a última camada utiliza a função de classificaçao Softmax.\n",
        "Por último, vemos um resumo do modelo que acabamos de construir. OBS: Lembre-se de mudar seu ambiente de execução para GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffrOFeKuL5nR"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "vocab_size = X_train.shape[0] + 1 \n",
        "\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#mostra o modelo construído\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw7A4Zm0gKn8"
      },
      "source": [
        "##Compilação do Modelo\n",
        "\n",
        "Nessa parte do código, definimos os algoritmos de otimização, a função de perda e a métrica que será utilizada para avaliação do modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybvzy_1RgJHc"
      },
      "source": [
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss = losses.binary_crossentropy,\n",
        "              metrics = ['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtq3pVoMg1qK"
      },
      "source": [
        "##Configuração da Avaliação\n",
        "Reservaremos uma parte de nossos dados de treinamento para validação da precisão do modelo durante o treinamento. Um conjunto de validação nos permite monitorar o progresso de nosso modelo em dados não vistos anteriormente à medida que ele passa por épocas durante o treinamento.\n",
        "As etapas de validação nos ajudam a ajustar os parâmetros de treinamento da função `model.fit` para evitar overfitting e underfitting de dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPKKuXGeMBY_"
      },
      "source": [
        "# Input for Validation\n",
        "X_val = X_train[:5000]\n",
        "partial_X_train = X_train[10000:]\n",
        "\n",
        "# Labels for validation\n",
        "y_val = y_train[:5000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLWSviV6iEWR"
      },
      "source": [
        "##Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BqT9ntbMEPf"
      },
      "source": [
        "history = model.fit(partial_X_train, \n",
        "                    partial_y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=512)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzeadSTcMUVe"
      },
      "source": [
        "### Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68aYM4fNMU61"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "preds = model.predict_classes(X_test)\n",
        "true  = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(metrics.classification_report(true, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvY8J3x8h31X"
      },
      "source": [
        "##Visualizando os Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR2mgZS7MJ8S"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "\n",
        "# Plotting losses\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss_values, 'b', label=\"Validation Loss\")\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEq-noiTOUY5"
      },
      "source": [
        "## Adicionando embeddings pré-treinados\n",
        "É possível usarmos embeddings pré-treinados. A escolha é sempre relativa ao seu problema. Por exemplo, se você precisa resolver um problema de classificação de texto de cunho geral, pode pegar um Embeddings pré-treinado do Google, com milhões de textos. Porém, se quiser resolver um problema de classificação de sentimentos de review de livros, pode ser útil utilizar um embeddings mais próximo do seu problema, como por exemplo, um embeddings pré-treinado com informações e review de livros da Amazon. Nessa atividade, vamos utilizar o Embedding de notícias do Google.\n",
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtI5eg7YOdQ2"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxJbj-nGOiew"
      },
      "source": [
        "### GLove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGeHCn84OlJM"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-2wE-W4OxZ_"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    \n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzn-_iqCO8R4"
      },
      "source": [
        "#Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1p7nyGtO-U3"
      },
      "source": [
        "1. Realize a análise de sentimentos com os Embeddings pré-treinados Word2Vec e Glove, utilizando a rede neural criada em cima. Na tabela abaixo, troque o X pelos resultados da acurácia, do precision, do recall e F1.\n",
        "\n",
        "\\begin{array}{|c|c|c|c|c|}\\hline \n",
        "  Embedding & Precisão & Recall & F1 & Acurácia\\\\  \\hline \n",
        "Word2Vec & X & X & X & X  \\\\ \\hline\n",
        "Glove  & X & X & X & X  \\\\ \\hline \n",
        "\\end{array}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWmUlHsyqVA1"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from sklearn import metrics\n",
        "\n",
        "#recupera palavras a partir dos indices da base imdb\n",
        "word_index = imdb.get_word_index()\n",
        "index_word = {k:v for k,v in word_index.items()}\n",
        "\n",
        "#Cria a matriz de Embeddings\n",
        "embedding_matrix = create_embedding_matrix('googlenews.word2vec.300d.txt',\n",
        "                      index_word, embedding_dim)\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "#Cria Rede Neural\n",
        "modelWord2Vec = Sequential()\n",
        "vocab_size = X_train.shape[0] + 1 \n",
        "\n",
        "modelWord2Vec.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "modelWord2Vec.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "modelWord2Vec.add(layers.GlobalMaxPooling1D())\n",
        "modelWord2Vec.add(layers.Dense(10, activation='relu'))\n",
        "modelWord2Vec.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#mostra o modelo construído\n",
        "#modelWord2Vec.summary()\n",
        "\n",
        "#Compila o Modelo \n",
        "modelWord2Vec.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss = losses.binary_crossentropy,\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#Treina o Modelo\n",
        "history = modelWord2Vec.fit(partial_X_train, \n",
        "                    partial_y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=512)\n",
        "\n",
        "#Avalia o Modelo\n",
        "loss, accuracy = modelWord2Vec.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = modelWord2Vec.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "preds = modelWord2Vec.predict_classes(X_test)\n",
        "true  = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(metrics.classification_report(true, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvLmo-PMPekx"
      },
      "source": [
        "2. Agora, crie uma nova rede neural e altere seus parâmetros para melhorar os resultados obtidos na questão anterior para os Embeddings pré-treinados Word2Vec e Glove. Mantenha o número de épocas em 10. \n",
        "\n",
        "  a) Escreva qual foi sua melhor alteração?  \n",
        "  b) Na tabela abaixo, troque o X pelos melhores resultados da acurácia, do precision, do recall e F1.\n",
        "\n",
        "\\begin{array}{|c|c|c|c|c|}\\hline \n",
        "  Embedding & Precisão & Recall & F1 & Acurácia\\\\  \\hline \n",
        "Word2Vec & X & X & X & X  \\\\ \\hline\n",
        "Glove  & X & X & X & X  \\\\ \\hline \n",
        "\\end{array}"
      ]
    }
  ]
}