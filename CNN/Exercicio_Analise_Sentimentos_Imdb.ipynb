{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05b_Sentiment_Analysis_Imdb.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/mineracao_textual/blob/main/CNN/Exercicio_Analise_Sentimentos_Imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLJozfTGU0Lj"
      },
      "source": [
        "##Análise de Sentimentos\n",
        "A análise de sentimento visa determinar a opinião ou sentimento das pessoas eem relçao a um produto, pessoa, evento, ou qualquer outro objeto de interesse. Por exemplo, um palestrante ou escritor em relação a um documento, interação ou evento. O sentimento é principalmente categorizado em categorias positivas, negativas e neutras. Por meio da análise de sentimento, podemos prever, por exemplo, a opinião e a atitude de um cliente sobre um produto com base em uma resenha que ele escreveu. Essa técnica é amplamente aplicada a coisas como revisões, pesquisas, documentos e muito mais.\n",
        "\n",
        "##Base de Dados IMDB\n",
        "O conjunto de dados de classificação de sentimento do IMDB (https://www.imdb.com/) consiste em 50000 resenhas de filmes de usuários do IMDB que são rotuladas como positivas (1) ou negativas (0). As 50000 resenhas são divididas em 25000 para treinamento e 25000 para teste. O conjunto de dados foi criado por pesquisadores da Universidade de Stanford e publicado em um artigo de 2011, onde alcançou 88.89% de precisão. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIxphGHVdu6w",
        "outputId": "a96aee2c-97e0-4135-ecf4-1249c96219d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "# Load the data, keeping only 10,000 of the most frequently occuring words\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oem7f6Prdu6_",
        "outputId": "12ecf5a7-3c4d-4ff7-8135-723181cd7204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Since we restricted ourselves to the top 10000 frequent words, no word index should exceed 10000\n",
        "# we'll verify this below\n",
        "# Here is a list of maximum indexes in every review --- we search the maximum index in this list of max indexes\n",
        "print(type([max(sequence) for sequence in train_data]))\n",
        "\n",
        "# Find the maximum of all max indexes\n",
        "max([max(sequence) for sequence in train_data])\n",
        "\n",
        "data = np.concatenate((train_data, test_data), axis=0)\n",
        "labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "\n",
        "print(\"Categories:\", np.unique(labels))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "Categories: [0 1]\n",
            "Number of unique words: 9998\n",
            "Average Review length: 234.75892\n",
            "Standard Deviation: 173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz1pTRryYWtb"
      },
      "source": [
        "Você pode ver acima que o conjunto de dados é classificado em duas categorias, 0 ou 1, que representa o sentimento da avaliação do filme, negativo e positivo respectivamente. Todo o conjunto de dados contém 9.998 palavras únicas e o comprimento médio da revisão é de 234 palavras, com um desvio padrão de 173 palavras. \n",
        "\n",
        "##Decodificação da Avaliação\n",
        "Abaixo, vamos decodificar o rótulo no formato *One-Hot Code*.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX33fS3rLltV",
        "outputId": "2e48385e-1154-413c-d2f4-c55b09fe783b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(labels.reshape(-1, 1))\n",
        "print(y[0])\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHeqvDdtaT8J"
      },
      "source": [
        "##Preparação dos Dados\n",
        "\n",
        "Nos códigos abaixo iremos realizar a separação dos dados em treinamento e teste utilizando a função `train_test_split` do sklearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOx2UvKtaynK",
        "outputId": "1f57ce64-8670-42f6-d62a-016cf5fdedb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences_train, sentences_test, y_train, y_test  = train_test_split(data, y, test_size=0.3, random_state=42, stratify=y)\n",
        "print(sentences_train[:2], y_train[:2])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([1, 193, 4, 1003, 1544, 592, 1648, 2, 7, 4, 4997, 1493, 11, 49, 5867, 39, 2, 2, 668, 1879, 2, 5203, 6485, 5, 9358, 11, 49, 2509, 2, 34, 167, 4748, 25, 28, 4, 132, 7521, 2493, 42, 4, 693, 3908, 7, 128, 573, 17, 4, 250, 39, 2, 11, 4, 2997, 2, 15, 132, 39, 2, 63, 47, 164, 8, 81, 19, 14, 422, 14, 127, 193, 273, 11, 3829, 75, 215, 202, 12, 15, 42, 2, 17, 442, 82, 5414, 8, 9, 40, 35, 445, 310, 7, 3960, 7375, 256, 133, 34, 524, 2, 19, 15, 1081, 2, 1826, 63, 91, 7, 178, 86, 877, 2, 19, 11, 2, 50, 26, 1942, 491, 56, 665, 7, 41, 4488, 125, 370, 1364, 636, 41, 1642, 242, 137, 149, 142, 4009, 190, 59, 9, 2, 11, 41, 1317, 2493, 136, 59, 7565, 35, 436, 1272, 7, 668, 5197, 2, 2, 5, 2269, 6, 543, 446, 42, 142, 43, 1005, 2, 852, 2, 134, 5867, 63, 2453, 1482, 11, 4, 1431, 523, 7, 481, 1307, 21, 466, 49, 1774, 270, 1592, 2057, 12, 1160, 4, 172, 2723, 841, 7, 18, 463, 4, 55, 1967, 49, 539, 81, 63, 385, 46, 187, 4, 172, 58, 5, 63, 82, 2561, 6, 668, 1272, 33, 86, 7334, 4, 1682, 7, 32, 134, 4475, 5186, 633, 2, 56, 11, 6, 3438, 4110, 2066, 602, 21, 103, 1119, 234, 42, 38, 25, 923, 225, 164, 334, 50, 724, 43, 956, 56, 4, 1114, 10, 10, 4, 114, 1160, 51, 186, 40, 6, 1003, 1544, 6, 915, 6812, 11, 3829, 19, 158, 1431, 2512, 29, 4110, 4, 692, 7, 4, 719, 823, 1635, 7071, 2, 12, 56, 17, 35, 3715, 1648, 552, 1019, 37, 3292, 465, 3941, 3959, 11, 2, 6088, 8, 2, 90, 14, 732, 285, 56, 18, 6, 342, 96, 1944, 200, 4, 1544, 4, 823, 1635, 5, 4, 1397, 4, 823, 1635, 494, 3908, 82, 941, 998, 4760, 11, 3062, 2, 279, 198, 31, 7, 4, 3908, 14, 934, 1127, 21, 50, 26, 712, 63, 140, 724, 43, 6, 550, 1062, 50, 26, 111, 665, 63, 100, 28, 343, 6, 176, 7, 2, 31, 324, 7, 35, 6812, 4614, 18, 463, 2677, 23, 4, 3914, 17, 12, 2, 8, 6, 751, 570, 17, 48, 14, 69, 115, 77, 2004, 23, 22, 159, 225, 6, 729, 1483, 8, 6, 801, 7384, 17, 48, 225, 142, 1732, 44, 12, 4, 548, 139, 26, 55, 2, 17, 48, 4, 1057, 69, 8, 361, 4, 86, 304, 8, 763, 49, 365, 1107, 225, 6, 136, 7, 4, 147, 2, 9110, 44, 2, 143, 146, 3099, 50, 71, 998, 352, 712, 262, 3523, 11, 4, 4541, 985, 121, 1214, 481, 302, 5, 3740, 1108, 56, 6, 176, 7, 6514, 206, 141, 17, 4, 583, 7, 60, 147, 267, 1865, 12, 2018, 8, 330, 148, 211, 54, 362, 361, 3161, 1865, 5, 3945, 3589, 26, 112, 3429, 1454, 120, 2, 50, 26, 2442, 7, 2, 141, 17, 8621, 6, 109, 8, 79, 2757, 5, 668, 1006, 35, 402, 2823, 7, 141, 103, 49, 2, 71, 5671, 21, 669, 490, 30, 2660, 129, 523, 632, 342, 1910, 457, 5203, 2, 457, 7904, 470, 1860, 342, 3289, 3227, 342, 2, 470, 7095, 470, 1979, 1086, 1062, 342, 444, 470])\n",
            " list([1, 13, 104, 13, 1176, 4, 4866, 4969, 18, 14, 20, 237, 13, 188, 12, 17, 173, 7, 6, 1647, 1733, 20, 2239, 288, 1594, 19, 1570, 102, 18, 891, 3596, 63, 817, 13, 1539, 142, 40, 1570, 6414, 18, 4, 580, 8, 106, 4, 328, 5552, 310, 7, 6, 20, 15, 16, 626, 17, 2, 10, 10, 18, 6, 1121, 1060, 509, 328, 5552, 218, 99, 78, 60, 151, 12, 9, 540, 2, 34, 6, 364, 352, 31, 7, 61, 2, 2269, 7, 8978, 18, 149, 6, 20, 9, 15, 48, 4, 485, 284, 9, 128, 74, 27, 365, 5, 881, 4, 20, 5347, 214, 33, 222, 289, 381, 15, 9, 434, 4, 420, 133, 2, 47, 49, 1369, 5, 49, 3315, 5, 242, 1816, 6, 128, 22, 611, 74, 29, 188, 10, 10, 4, 890, 548, 3822, 4, 2, 282, 18, 4, 22, 66, 528, 8, 4171, 259, 37, 47, 126, 11, 6, 1647, 1733, 396, 42, 60, 43, 77, 2, 11, 6, 2, 548, 13, 64, 1084, 44, 107, 153, 2799, 1121, 2135, 1879, 21, 60, 13, 62, 115, 809, 18, 4, 1011, 2, 3415, 3131, 3465, 5, 2, 5904, 23, 2444, 133, 21, 4, 841, 9, 52, 4705, 5, 541, 5, 5763, 9909, 5, 4, 156, 276, 49, 547, 83, 4, 548, 139, 10, 10, 329, 867, 9, 4, 114, 2, 109, 2, 9, 1000, 17, 6, 890, 1393, 2455, 185, 132, 37, 271, 267, 18, 157, 17, 6, 1261, 56, 4025, 11, 4667, 890, 4259, 246, 29, 9, 770, 5, 2, 54, 29, 47, 8, 990, 125, 4, 1788, 1338, 72, 21, 13, 16, 2841, 11, 392, 513, 2, 5, 60, 13, 697, 39, 149, 254, 211, 19, 1394, 6182, 15, 4, 1788, 28, 8, 30, 1539, 125, 18, 14, 432, 7, 206, 5, 15, 4, 493, 37, 548, 887, 4, 11, 661, 8, 79, 68, 4259, 5, 15, 4, 71, 290, 4, 278, 38, 25, 28, 8, 106, 14, 20, 19, 6, 432, 7, 2, 5488, 7, 129, 2778, 2, 11, 661, 8, 1779, 12, 17, 6, 328, 597, 112, 6310, 20, 91, 7, 4, 85, 701, 328, 8438, 11, 4, 9707, 79, 2, 11, 4, 393, 18, 4, 172, 855, 99, 422, 10, 10, 4, 20, 2478, 39, 6, 346, 692, 6312, 4, 167, 540, 161, 28, 4, 352, 8, 22, 49, 7, 4, 139, 29, 887, 38, 29, 69, 8, 2233, 11, 4, 7346, 19, 49, 1018, 2761, 5302, 139, 4, 13, 90, 13, 90, 5, 95, 13, 556, 90, 136, 43, 152, 157, 367, 19, 544, 7363, 5, 9474, 15, 26, 4116, 5, 2631, 14, 9, 262, 283, 19, 4, 226, 883, 2651, 63, 186, 8, 28, 77, 814, 17, 48, 12, 71, 35, 2, 14, 9, 6, 117, 5360, 54, 25, 1133, 15, 4, 341, 7, 5078, 341, 9, 424, 8, 7374, 27, 1278, 18, 1060, 10, 10, 21, 280, 4, 20, 8763, 32, 4, 96, 39, 185, 4025, 4683, 143, 4, 3897, 8, 4, 1060, 756, 12, 2865, 56, 6, 117, 6487, 5, 299, 19, 6, 117, 53, 5380, 146, 24, 252, 15, 4, 477, 7447, 9, 290, 4, 2, 2477, 6451, 2018, 14, 432, 7, 155, 6, 196, 1278, 18, 6, 346, 251, 33, 4, 2583, 21, 12, 127, 4228, 183, 125, 11, 6, 3716, 2349, 96, 10, 10, 48, 2, 5811, 69, 93, 14, 22, 19, 6, 147, 352, 5, 4, 172, 177, 2, 732, 5, 1352, 5, 309, 17, 4, 632, 84, 62, 28, 2, 12, 17, 4, 375, 4811, 63, 271, 8, 123, 25, 89, 2, 5, 580, 70, 297, 6656, 19, 62, 30, 2, 1306, 10, 10, 290, 319, 280, 18, 998, 542, 665, 5, 411, 5, 8, 106, 2, 9638, 9343, 309, 11, 6, 217, 15, 9, 4158, 90])] [[1. 0.]\n",
            " [1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClM3xhZffWMC"
      },
      "source": [
        "## Sequence Padding\n",
        "\n",
        "Um problema que temos é que cada sequência de texto tem na maioria dos casos diferentes comprimentos de palavras. Para corrigir isso, vamos usar `pad_sequence()` que simplesmente preenche a sequência de palavras com zeros. O número máximo 250 foi escolhido por uma aproximação da média de palavras na avaliação que é 234. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgdxOi8ja7tb",
        "outputId": "25d26e42-a368-4fa4-d364-e3819ad9e7a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 250\n",
        "X_train = pad_sequences(sentences_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(sentences_test, padding='post', maxlen=maxlen)\n",
        "print(X_train[100])\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1   14    9   31    7   61   32   58 1640  108   94   31    7  148\n",
            "  108  121   13  124  175  347   21   70  131  106   12 3727  209 2328\n",
            "  602   13  210 1402   23   14   22   48  146  170   23 3182   42   48\n",
            "   13   92  181    8  140    8    4 9837   43  319 1490  648   80  202\n",
            "   72    4 3666   13  359   94   35  776    8  106   22   63  210  941\n",
            "   72 4783   21   13  124   12 4283   30    2 3647    7 3384   21   48\n",
            "   40   72   25  119  108   15   26  324 1293    5   28  212  883    5\n",
            "   35  221  114   25   80  119   12   12    9 1061   19   87  105    5\n",
            " 1022    5    2   26  199 1490   38  259   70 6157   33  142 2276 1324\n",
            "  347   25   28    8  202   12    6  353   13  296   12   23  248   31\n",
            "  251   34    2    5  447   12   15   13   69    8  140   46    5 1406\n",
            "   12  180   63  562   49   58    5   13  100  106   12 2905   61  336\n",
            "    4  632   13  119   25    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZso-lvrdhHW"
      },
      "source": [
        "##Construindo a Rede Neural\n",
        "\n",
        "Agora podemos construir uma rede neural simples. Começaremos definindo o tipo de modelo que queremos construir. Existem dois tipos de modelos disponíveis no Keras: o modelo sequencial e a classe do modelo usado na API funcional.\n",
        "\n",
        "Começamos adicionando a primeira camada padrão ao lidar com Texto e CNNs: a camada de Embeddings. Nesse exercício vamos usar um Embedding de tamanho 300. Depois, camadas de Convolução e MaxPooling são adicionadas e por fim, camadas de classificação Densas, sendo que a última camada utiliza a função de classificaçao Softmax.\n",
        "Por último, vemos um resumo do modelo que acabamos de construir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffrOFeKuL5nR",
        "outputId": "bd1a92f7-765b-496f-8dc8-01b34dfa9010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "vocab_size = X_train.shape[0] + 1 \n",
        "\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#mostra o modelo construído\n",
        "model.summary()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 250, 300)          10500300  \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 246, 32)           48032     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_6 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 10,548,684\n",
            "Trainable params: 10,548,684\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw7A4Zm0gKn8"
      },
      "source": [
        "##Compilação do Modelo\n",
        "\n",
        "Nessa parte do código, definimos os algoritmos de otimização, a função de perda e a métrica que será utilizada para avaliação do modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybvzy_1RgJHc",
        "outputId": "f208b642-cbe8-43c6-aae9-3960f340cfb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss = losses.binary_crossentropy,\n",
        "              metrics = [metrics.binary_accuracy])\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-fe7dc87028cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n\u001b[1;32m      2\u001b[0m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m               metrics = [metrics.binary_accuracy])\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn.metrics' has no attribute 'binary_accuracy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtq3pVoMg1qK"
      },
      "source": [
        "##Configuração da Avaliação\n",
        "Reservaremos uma parte de nossos dados de treinamento para validação da precisão do modelo durante o treinamento. Um conjunto de validação nos permite monitorar o progresso de nosso modelo em dados não vistos anteriormente à medida que ele passa por épocas durante o treinamento.\n",
        "As etapas de validação nos ajudam a ajustar os parâmetros de treinamento da função `model.fit` para evitar overfitting e underfitting de dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPKKuXGeMBY_"
      },
      "source": [
        "# Input for Validation\n",
        "X_val = X_train[:5000]\n",
        "partial_X_train = X_train[10000:]\n",
        "\n",
        "# Labels for validation\n",
        "y_val = y_train[:5000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLWSviV6iEWR"
      },
      "source": [
        "##Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BqT9ntbMEPf"
      },
      "source": [
        "history = model.fit(partial_X_train, \n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=512)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzeadSTcMUVe"
      },
      "source": [
        "### Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68aYM4fNMU61"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "preds = model.predict_classes(X_test)\n",
        "true  = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(metrics.classification_report(true, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvY8J3x8h31X"
      },
      "source": [
        "##Visualizando os Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR2mgZS7MJ8S"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "\n",
        "# Plotting losses\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss_values, 'b', label=\"Validation Loss\")\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEq-noiTOUY5"
      },
      "source": [
        "## Adicionando embeddings pré-treinados\n",
        "É possível usarmos embeddings pré-treinados. A escolha é sempre relativa ao seu problema. Por exemplo, se você precisa resolver um problema de classificação de texto de cunho geral, pode pegar um Embeddings pré-treinado do Google, com milhões de textos. Porém, se quiser resolver um problema de classificação de sentimentos de review de livros, pode ser útil utilizar um embeddings mais próximo do seu problema, como por exemplo, um embeddings pré-treinado com informações e review de livros da Amazon. Nessa atividade, vamos utilizar....\n",
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtI5eg7YOdQ2"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8il41FkOf2S"
      },
      "source": [
        "!head -n 1 googlenews.word2vec.300d.txt | cut -c-50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxJbj-nGOiew"
      },
      "source": [
        "### GLove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGeHCn84OlJM"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZSeUUOsOm4U"
      },
      "source": [
        "!head -n 1 glove.840B.300d.sst.txt | cut -c-50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-2wE-W4OxZ_"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzn-_iqCO8R4"
      },
      "source": [
        "#Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1p7nyGtO-U3"
      },
      "source": [
        "1. Avalie a atividade com Embeddings pré-treinados Word2Vec e Glove utilizando a rede neural criada em cima. Escreva o resultado da acurácia, do precision, do recall e F1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvLmo-PMPekx"
      },
      "source": [
        "2. Agora, crie uma nova rede neural e altere seus parâmetros para melhorar os resultados obtidos na questão anterior para os Embeddings pré-treinados Word2Vec e Glove. Mantenha o número de épocas em 20. \n",
        "\n",
        "  a) Escreva qual foi sua alteração?  \n",
        "  b) Escreva o resultado da acurácia, do precision, do recall e F1."
      ]
    }
  ]
}