{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_CNN_TextClassification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/mineracao_textual/blob/main/CNN/05_CNN_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGPx2I9UyB0t"
      },
      "source": [
        "!git clone https://github.com/valmirf/mineracao_textual.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLCeIJRCGM4s"
      },
      "source": [
        "# Classificação de Textos com CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFFknubSGeC_"
      },
      "source": [
        "## Carregar Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPMfiTT9y4p"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = data = pd.read_csv(\"mineracao_textual/Dados/ClothingReviews.csv\")\n",
        "\n",
        "# descartar classes minoritarias\n",
        "major_classes = list(df['Class Name'].value_counts()[0:4].index)\n",
        "majority      = df.loc[df['Class Name'].isin(major_classes),]\n",
        "minority      = df.loc[~df['Class Name'].isin(major_classes),]\n",
        "\n",
        "print('\\n',5*'=', 'major classes: \\n',    majority['Class Name'].value_counts()[0:4])\n",
        "print('\\n',5*'=', 'minority classes: \\n', minority['Class Name'].value_counts()[4:])\n",
        "\n",
        "majority.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjZ4Q5LSGmLi"
      },
      "source": [
        "## Codificar Labels e Separar treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZFGpNA3-ppf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "_labels = majority['Class Name'].values.reshape((len(majority['Class Name']), 1))\n",
        "X = majority['Review Text']\n",
        "y = encoder.fit_transform(_labels)\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test  = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDQrrr-V_XqJ"
      },
      "source": [
        "sentences_train[:4], y_train[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNp-bp4REldT"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86mnIfM4Gw8o"
      },
      "source": [
        "## Tokenizando Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-JGH1BV-bzh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train   = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test    = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "\n",
        "print(vocab_size)\n",
        "print(sentences_train.iloc[2])\n",
        "print(X_train[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgf1QWbGV0A7"
      },
      "source": [
        "#X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmbodUTYEu5w"
      },
      "source": [
        "#list(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxD8N8AeG2bF"
      },
      "source": [
        "## Sequence Padding\n",
        "\n",
        "Um problema que temos é que cada sequência de texto tem na maioria dos casos diferentes comprimentos de palavras. Para corrigir isso, você pode usar `pad_sequence()` que simplesmente preenche a sequência de palavras com zeros. \n",
        "\n",
        "Por padrão, ele anexa zeros, mas queremos anexá-los. Normalmente, não importa se você acrescenta ou acrescenta zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McfGfFYo_CUB"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train[0, :], y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TFkefspHOeB"
      },
      "source": [
        "## Arquitetura da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RMnG0lEAvLr"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpY9AnPhA07I"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koIALEmzHabN"
      },
      "source": [
        "## Adicionando embeddings pré-treinados\n",
        "É possível usarmos embeddings pré-treinados. A escolha é sempre relativa ao seu problema. Por exemplo, se você precisa resolver um problema de classificação de texto de cunho geral, pode pegar um Embeddings pré-treinado do Google, com milhões de textos. Porém, se quiser resolver um problema de classificação de sentimentos de review de livros, pode ser útil utilizar um embeddings mais próximo do seu problema, como por exemplo, um embeddings pré-treinado com informações e review de livros da Amazon. Nessa atividade, vamos utilizar....\n",
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzOVZ2vzCXQC"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-D4hRM2CV7y"
      },
      "source": [
        "!head -n 1 googlenews.word2vec.300d.txt | cut -c-50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRPhGLTDHo8r"
      },
      "source": [
        "### GLove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eqJimlgHrAe"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHiLHMT6HuKF"
      },
      "source": [
        "!head -n 1 glove.840B.300d.sst.txt | cut -c-50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyXEVU1H3Ka"
      },
      "source": [
        "## Teste: Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkMbEWw6A34u"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "    \n",
        "embedding_matrix = create_embedding_matrix('googlenews.word2vec.300d.txt',\n",
        "                      tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYtX9OjZDLXj"
      },
      "source": [
        "embedding_matrix[0:3,:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psvpHf34DRwL"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ2p3j8jDaUY"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7bDmmx5H9Ft"
      },
      "source": [
        "## Teste: Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41qSXd0sDc7w"
      },
      "source": [
        "embedding_matrix = create_embedding_matrix('glove.840B.300d.sst.txt',\n",
        "                      tokenizer.word_index, embedding_dim)\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2HNzu7fF-Eq"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2S9rGT-JRIW"
      },
      "source": [
        "## Métricas detalhadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KADwY35dINUf"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "y_true  = np.argmax(y_test, axis=1)\n",
        "preds = (model.predict(X_test) > 0.5).astype(\"int32\")[:, [1]]\n",
        "\n",
        "print(metrics.classification_report(y_true,preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK3YTYJ5Jfnd"
      },
      "source": [
        "## Como melhorar?\n",
        "\n",
        "- Hyperparameter tuning\n",
        "- Adicionar mais filtros (convolution layers), por exemplo:\n",
        "\n",
        "```\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 512\n",
        "\n",
        "...\n",
        "\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "\n",
        "...\n",
        "```"
      ]
    }
  ]
}