{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_CNN_TextClassification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/mineracao_textual/blob/main/CNN/05_CNN_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGPx2I9UyB0t",
        "outputId": "aee52e51-9578-4025-9e94-8538129da91f"
      },
      "source": [
        "!git clone https://github.com/valmirf/mineracao_textual.git"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mineracao_textual' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLCeIJRCGM4s"
      },
      "source": [
        "# Classificação de Textos com CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFFknubSGeC_"
      },
      "source": [
        "## Carregar Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPMfiTT9y4p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "c0b4385d-cfcd-46b3-905a-8184c6e2e21c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = data = pd.read_csv(\"mineracao_textual/Dados/dataset_ness_law.csv\")\n",
        "\n",
        "# descartar classes minoritarias\n",
        "major_classes = list(df['objeto'].value_counts()[0:4].index)\n",
        "majority      = df.loc[df['objeto'].isin(major_classes),]\n",
        "minority      = df.loc[~df['objeto'].isin(major_classes),]\n",
        "\n",
        "print('\\n',5*'=', 'major classes: \\n',    majority['objeto'].value_counts()[0:4])\n",
        "print('\\n',5*'=', 'minority classes: \\n', minority['objeto'].value_counts()[4:])\n",
        "\n",
        "majority.head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ===== major classes: \n",
            " Atraso ou Cancelamento de Voo    136\n",
            "Alteração na Malha Aérea          79\n",
            "Doméstico                         59\n",
            "Extravio de Bagagem               49\n",
            "Name: objeto, dtype: int64\n",
            "\n",
            " ===== minority classes: \n",
            " Provisório                      8\n",
            "Erro ou Alteração na Reserva    6\n",
            "Outros                          3\n",
            "Name: objeto, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>txt</th>\n",
              "      <th>objeto</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fatos joão césar sala eldorado contagem marque...</td>\n",
              "      <td>Extravio de Bagagem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fatos fatos ocorridos trecho conforme document...</td>\n",
              "      <td>Doméstico</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fatos autores presente ação programaram meses ...</td>\n",
              "      <td>Atraso ou Cancelamento de Voo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fatos objetivando viajar porto alegre particip...</td>\n",
              "      <td>Alteração na Malha Aérea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fatos autor comprou passagem retorno casa após...</td>\n",
              "      <td>Extravio de Bagagem</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 txt                         objeto\n",
              "0  fatos joão césar sala eldorado contagem marque...            Extravio de Bagagem\n",
              "1  fatos fatos ocorridos trecho conforme document...                      Doméstico\n",
              "2  fatos autores presente ação programaram meses ...  Atraso ou Cancelamento de Voo\n",
              "3  fatos objetivando viajar porto alegre particip...       Alteração na Malha Aérea\n",
              "4  fatos autor comprou passagem retorno casa após...            Extravio de Bagagem"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjZ4Q5LSGmLi"
      },
      "source": [
        "## Codificar Labels e Separar treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZFGpNA3-ppf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "_labels = majority['objeto'].values.reshape((len(majority['objeto']), 1))\n",
        "X = majority['txt']\n",
        "y = encoder.fit_transform(_labels)\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test  = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDQrrr-V_XqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141ac50f-f26b-4fe0-af78-c273119d8e3f"
      },
      "source": [
        "sentences_train[:4], y_train[:4]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(107    fatos autora cliente requerida utilizou serviç...\n",
              " 31     fatos atraso injustificado perda conexão chega...\n",
              " 342    fatos requerente planejou realizar viagem dest...\n",
              " 176    fatos requerente adquiriu passagem realizar vi...\n",
              " Name: txt, dtype: object, array([[0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNp-bp4REldT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ea93d1-2d39-4e81-eb0a-ec34af1e822f"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(226, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86mnIfM4Gw8o"
      },
      "source": [
        "## Tokenizando Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-JGH1BV-bzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4f775f-e92e-4982-fe96-4cb7831b6e7e"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train   = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test    = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "\n",
        "print(vocab_size)\n",
        "print(sentences_train.iloc[2])\n",
        "print(X_train[2])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18463\n",
            "fatos requerente planejou realizar viagem destino fernando noronha objetivo efetuar mergulho grande profundidade participar jogo futebol junto amigos jiqui country club escolhido destino procurou agência viagem brasil operadora agência viagens meio realizou compra passagens aéreas quais ficaram importe setecentos vinte reais trinta centavos sendo empresa azul linhas aéreas brasileiras requerida companhia escolhida anexo passagem marcada saída aeroporto recife chegada fernando noronha questão companhia azul linhas aéreas brasileiras anexo volta marcada ocorre requerente chegou aeroporto volta check ficou aguardando entanto somente volta informado havia sido cancelado inclusive neste meio tempo saiu marcado mesma companhia aérea destino vogado apesar anúncio sido cancelado ocorrido volta requerente ficou aguardando posicionamento companhia saber iria embarcar somente volta companhia chamou requerente voucher alimentação valor pudesse almoçar disse após almoço retornasse sala embarque levado hotel pois sairia naquele solicitado almoçar requerente voltou sala embarque aguardou volta levado hotel oportunidade informado daquele iriam informar horário requerente embarcaria bastasse aborrecimento ficar aeroporto horas aguardando posicionamento companhia aérea requerente ainda recebeu voucher alimentação valor insuficiente refeição restaurante indicado companhia anexo somente após voltar hotel requerente informado sairia seguinte chegada fernando noronha ocorre relatado requerente fernando noronha dois objetivos participar jogo futebol praticar mergulho grande profundidade considerado melhor mundo nesta categoria devido cancelamento requerente ficou impossibilitado praticar mergulho grande profundidade pois recomenda após mergulho praticante somente poderá voar após mínimo sendo aconselhado correr risco nenhum anexo visitante chegaria fernando noronha feira praticaria mergulho feira voltaria recife devido mudança requerente chegou fernando noronha feira então poderia realizar mergulho sábado porém arriscado pois voar volta recife colocaria vida risco desta forma pode realizar sonhado mergulho profundidade famoso arquipélago fernando noronha frise considerado melhor mergulho mundo nesta categoria bastassem danos relatados tópicos anteriores importante destacar requerente ainda perdeu diária hospedagem pousada estação noronha paga importe diante exposto considerando fatos aqui narrados evidentes danos materiais extrapatrimoniais sofridos requerente resultaram ajuizamento presente ação\n",
            "[41, 1, 1653, 158, 8, 17, 417, 456, 1060, 1936, 2992, 108, 4659, 1099, 4660, 248, 1411, 995, 17, 1526, 326, 8, 161, 2747, 326, 360, 258, 824, 168, 74, 59, 466, 801, 1817, 1937, 407, 72, 542, 262, 32, 6, 15, 109, 59, 283, 5, 33, 3632, 57, 46, 825, 103, 9, 60, 47, 417, 456, 578, 33, 15, 109, 59, 283, 57, 48, 825, 170, 1, 174, 9, 48, 112, 292, 494, 351, 69, 48, 255, 50, 87, 172, 151, 213, 258, 84, 1061, 692, 160, 33, 19, 17, 1022, 4661, 87, 172, 291, 48, 1, 292, 494, 1818, 33, 776, 387, 245, 69, 48, 33, 1, 560, 235, 14, 1260, 2194, 1527, 43, 1412, 80, 29, 2195, 101, 26, 709, 802, 1658, 2194, 1, 80, 29, 1938, 48, 2195, 101, 622, 255, 1220, 1358, 777, 38, 1, 1659, 1359, 467, 475, 9, 11, 494, 1818, 33, 19, 1, 31, 584, 560, 235, 14, 2993, 1307, 2994, 2052, 33, 57, 69, 43, 2196, 101, 1, 255, 709, 92, 47, 417, 456, 170, 2197, 1, 417, 456, 259, 2349, 1099, 4660, 1819, 2992, 108, 4659, 1464, 400, 756, 468, 2053, 263, 42, 1, 292, 4662, 1819, 2992, 108, 4659, 26, 43, 2992, 69, 562, 2350, 43, 803, 32, 4663, 419, 324, 57, 708, 417, 456, 204, 2992, 204, 60, 263, 361, 1, 174, 417, 456, 204, 173, 273, 158, 2992, 1097, 249, 26, 2350, 48, 60, 233, 419, 139, 40, 159, 158, 2992, 4659, 2351, 417, 456, 1820, 1464, 400, 2992, 756, 468, 2053, 7, 1932, 4664, 370, 778, 1, 31, 693, 971, 433, 2748, 456, 1590, 1817, 67, 327, 495, 41, 457, 779, 2749, 7, 125, 3633, 210, 1, 2524, 58, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgf1QWbGV0A7"
      },
      "source": [
        "#X_train"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmbodUTYEu5w"
      },
      "source": [
        "#list(tokenizer.word_index)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxD8N8AeG2bF"
      },
      "source": [
        "## Sequence Padding\n",
        "\n",
        "Um problema que temos é que cada sequência de texto tem na maioria dos casos diferentes comprimentos de palavras. Para corrigir isso, você pode usar `pad_sequence()` que simplesmente preenche a sequência de palavras com zeros. \n",
        "\n",
        "Por padrão, ele anexa zeros, mas queremos anexá-los. Normalmente, não importa se você acrescenta ou acrescenta zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McfGfFYo_CUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc8acd5-0fdc-45ea-f17a-19ec5d3e15e7"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train[0, :], y_train[0])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2049  213 1460 4651    2    6    5  322    2  605  325   46  417  456\n",
            "   18   37   57  155    6    5  275    2  398  129   46  417  456  730\n",
            "  231   43    8    6    5  426   50   87  323 1352  560    2   95  299\n",
            " 1303    2 2345    8  160    2 1304 4652 2742  111  168 1520   15  304\n",
            "  111   50   87 1522    8 1353   90  193  282    1   77   87 2519   13\n",
            "   34 3622 2519   13 1732  336  124   43   64  633  287 2987 1586   86\n",
            "  359    5  146   43   56  800 1932    2  332   82    7   28  125 2743\n",
            "    6 4649] [0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TFkefspHOeB"
      },
      "source": [
        "## Arquitetura da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RMnG0lEAvLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a8cf2b-cea9-4918-96fe-02ad4f514825"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 200\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Conv1D(300, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_19 (Embedding)     (None, 100, 200)          3692600   \n",
            "_________________________________________________________________\n",
            "conv1d_19 (Conv1D)           (None, 96, 300)           300300    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_19 (Glo (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 10)                3010      \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 4)                 44        \n",
            "=================================================================\n",
            "Total params: 3,995,954\n",
            "Trainable params: 3,995,954\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpY9AnPhA07I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb39c24-d35a-490f-977d-d7067eebe5c3"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.0000\n",
            "Testing Accuracy:  0.5155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koIALEmzHabN"
      },
      "source": [
        "## Adicionando embeddings pré-treinados\n",
        "É possível usarmos embeddings pré-treinados. A escolha é sempre relativa ao seu problema. Por exemplo, se você precisa resolver um problema de classificação de texto de cunho geral, pode pegar um Embeddings pré-treinado do Google, com milhões de textos. Porém, se quiser resolver um problema de classificação de sentimentos de review de livros, pode ser útil utilizar um embeddings mais próximo do seu problema, como por exemplo, um embeddings pré-treinado com informações e review de livros da Amazon. Nessa atividade, vamos utilizar....\n",
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzOVZ2vzCXQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b622ac5-1d20-459a-8e36-3937b437132e"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-28 00:57:15--  https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66209703 (63M) [text/plain]\n",
            "Saving to: ‘googlenews.word2vec.300d.txt.3’\n",
            "\n",
            "googlenews.word2vec 100%[===================>]  63.14M   191MB/s    in 0.3s    \n",
            "\n",
            "2021-04-28 00:57:15 (191 MB/s) - ‘googlenews.word2vec.300d.txt.3’ saved [66209703/66209703]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-D4hRM2CV7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67e31e4-1fea-41c8-974b-5a19283a9bd1"
      },
      "source": [
        "!head -n 1 googlenews.word2vec.300d.txt | cut -c-50"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in 0.0703125 0.08691406 0.087890625 0.0625 0.06933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRPhGLTDHo8r"
      },
      "source": [
        "### GLove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eqJimlgHrAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b91ab95a-a3ee-4819-be25-1b874ae9a7a6"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-28 00:57:15--  https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53933315 (51M) [text/plain]\n",
            "Saving to: ‘glove.840B.300d.sst.txt.3’\n",
            "\n",
            "glove.840B.300d.sst 100%[===================>]  51.43M  82.4MB/s    in 0.6s    \n",
            "\n",
            "2021-04-28 00:57:16 (82.4 MB/s) - ‘glove.840B.300d.sst.txt.3’ saved [53933315/53933315]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHiLHMT6HuKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a52fac-7a04-4a96-e428-2cb0a104f394"
      },
      "source": [
        "!head -n 1 glove.840B.300d.sst.txt | cut -c-50"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ", -0.082752 0.67204 -0.14987 -0.064983 0.056491 0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyXEVU1H3Ka"
      },
      "source": [
        "## Teste: Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkMbEWw6A34u"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "    \n",
        "embedding_matrix = create_embedding_matrix('googlenews.word2vec.300d.txt',\n",
        "                      tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYtX9OjZDLXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b0ffb0-cedf-4cde-8f66-b5b1196efbd6"
      },
      "source": [
        "embedding_matrix[0:3,:10]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psvpHf34DRwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825b6da0-de01-466e-8d7b-a4d84eaae267"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 100, 200)          3692600   \n",
            "_________________________________________________________________\n",
            "conv1d_12 (Conv1D)           (None, 96, 128)           128128    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_12 (Glo (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 4)                 44        \n",
            "=================================================================\n",
            "Total params: 3,822,062\n",
            "Trainable params: 129,462\n",
            "Non-trainable params: 3,692,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ2p3j8jDaUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87314120-bcc8-4d21-9425-399d19de1cc9"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.8363\n",
            "Testing Accuracy:  0.4021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7bDmmx5H9Ft"
      },
      "source": [
        "## Teste: Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41qSXd0sDc7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34e97c0-c991-4b7c-a7fd-1a36598682df"
      },
      "source": [
        "embedding_matrix = create_embedding_matrix('glove.840B.300d.sst.txt',\n",
        "                      tokenizer.word_index, embedding_dim)\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 100, 200)          3692600   \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, 96, 128)           128128    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_13 (Glo (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 4)                 44        \n",
            "=================================================================\n",
            "Total params: 3,822,062\n",
            "Trainable params: 129,462\n",
            "Non-trainable params: 3,692,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2HNzu7fF-Eq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638f922b-12f5-4528-e69e-d5039b2cacfe"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.8451\n",
            "Testing Accuracy:  0.3608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2S9rGT-JRIW"
      },
      "source": [
        "## Métricas detalhadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KADwY35dINUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0622a1ec-a26b-4cb7-d3f2-558af5cc10a7"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "preds = model.predict_classes(X_test)\n",
        "true  = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(metrics.classification_report(true, preds))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.29      0.27        24\n",
            "           1       0.51      0.56      0.53        41\n",
            "           2       0.12      0.06      0.08        18\n",
            "           3       0.25      0.29      0.27        14\n",
            "\n",
            "    accuracy                           0.36        97\n",
            "   macro avg       0.28      0.30      0.29        97\n",
            "weighted avg       0.34      0.36      0.35        97\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK3YTYJ5Jfnd"
      },
      "source": [
        "## Como melhorar?\n",
        "\n",
        "- Hyperparameter tuning\n",
        "- Adicionar mais filtros (convolution layers), por exemplo:\n",
        "\n",
        "```\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 512\n",
        "\n",
        "...\n",
        "\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "\n",
        "...\n",
        "```"
      ]
    }
  ]
}