{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05b_Sentiment_Analysis_Imdb.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/mineracao_textual/blob/main/CNN/05b_Sentiment_Analysis_Imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLJozfTGU0Lj"
      },
      "source": [
        "##Análise de Sentimentos\n",
        "A análise de sentimento visa determinar a opinião ou sentimento das pessoas eem relçao a um produto, pessoa, evento, ou qualquer outro objeto de interesse. Por exemplo, um palestrante ou escritor em relação a um documento, interação ou evento. O sentimento é principalmente categorizado em categorias positivas, negativas e neutras. Por meio da análise de sentimento, podemos prever, por exemplo, a opinião e a atitude de um cliente sobre um produto com base em uma resenha que ele escreveu. Essa técnica é amplamente aplicada a coisas como revisões, pesquisas, documentos e muito mais.\n",
        "\n",
        "##Base de Dados IMDB\n",
        "O conjunto de dados de classificação de sentimento do IMDB (https://www.imdb.com/) consiste em 50000 resenhas de filmes de usuários do IMDB que são rotuladas como positivas (1) ou negativas (0). As 50000 resenhas são divididas em 25000 para treinamento e 25000 para teste. O conjunto de dados foi criado por pesquisadores da Universidade de Stanford e publicado em um artigo de 2011, onde alcançou 88.89% de precisão. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIxphGHVdu6w"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "# Load the data, keeping only 10,000 of the most frequently occuring words\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oem7f6Prdu6_"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Since we restricted ourselves to the top 10000 frequent words, no word index should exceed 10000\n",
        "# we'll verify this below\n",
        "# Here is a list of maximum indexes in every review --- we search the maximum index in this list of max indexes\n",
        "print(type([max(sequence) for sequence in train_data]))\n",
        "\n",
        "# Find the maximum of all max indexes\n",
        "max([max(sequence) for sequence in train_data])\n",
        "\n",
        "data = np.concatenate((train_data, test_data), axis=0)\n",
        "labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "print(\"Categories:\", np.unique(labels))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz1pTRryYWtb"
      },
      "source": [
        "Você pode ver acima que o conjunto de dados é classificado em duas categorias, 0 ou 1, que representa o sentimento da resenha, negativo e positivo respectivamente. Todo o conjunto de dados contém 9.998 palavras únicas e o comprimento médio da revisão é de 234 palavras, com um desvio padrão de 173 palavras. \n",
        "\n",
        "##Decodificação da Resenha\n",
        "Abaixo, podemos ver a primeira revisão do conjunto de dados, que é rotulada como positiva (1). O primeiro passo é mapear as palavras em índices inteiros. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX33fS3rLltV"
      },
      "source": [
        "# Let's quickly decode a review\n",
        "\n",
        "# step 1: load the dictionary mappings from word to integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# step 2: reverse word index to map integer indexes to their respective words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# Step 3: decode the review, mapping integer indices to words\n",
        "#\n",
        "# indices are off by 3 because 0, 1, and 2 are reserverd indices for \"padding\", \"Start of sequence\" and \"unknown\"\n",
        "decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])\n",
        "\n",
        "decoded_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHeqvDdtaT8J"
      },
      "source": [
        "##Preparação dos Dados\n",
        "\n",
        "O primeiro passo é converter os índices em Tensores para usar na biblioteca do TensorFlow. O vetor de 10000 dimensões correspondente a cada revisão terá um índice correspondente a uma palavra. Todo índice com valor 1 é uma palavra que está presente na revisão e cada índice contendo 0 é uma palavra que não está presente na revisão. Isso resultará em um tensor de tamanho (25000, 10000)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVDfXjZiLrki"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))    # Creates an all zero matrix of shape (len(sequences),10K)\n",
        "    for i,sequence in enumerate(sequences):\n",
        "        results[i,sequence] = 1                        # Sets specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Vectorize training Data\n",
        "X_train = vectorize_sequences(train_data)\n",
        "\n",
        "# Vectorize testing Data\n",
        "X_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1dQLNTrLxE2"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUVFQUOSLzhN"
      },
      "source": [
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test  = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZso-lvrdhHW"
      },
      "source": [
        "##Construindo a Rede Neural\n",
        "\n",
        "Agora podemos construir uma rede neural simples. Começaremos definindo o tipo de modelo que queremos construir. Existem dois tipos de modelos disponíveis no Keras: o modelo sequencial e a classe do modelo usado na API funcional.\n",
        "\n",
        "Em seguida, simplesmente adicionamos as camadas de entrada, ocultas e de saída. Entre eles, estamos usando o *dropout* para evitar o sobreajuste. \n",
        "\n",
        "Usaremos a camada \"*dense*\", o que significa que as unidades estão totalmente conectadas. Dentro das camadas ocultas, usamos a função relu porque é sempre um bom começo e produz um resultado satisfatório na maioria das vezes. Sinta-se à vontade para experimentar outras funções de ativação.\n",
        "\n",
        "Na camada de saída, usamos a função sigmóide, que mapeia os valores entre 0 e 1. Observe que definimos a forma de entrada para 10000 na camada de entrada porque nossas análises têm 10000 inteiros de comprimento. A camada de entrada recebe 10000 como entrada e a produz com um formato de 50.\n",
        "\n",
        "Por último, vemos um resumo do modelo que acabamos de construir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffrOFeKuL5nR"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(model)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw7A4Zm0gKn8"
      },
      "source": [
        "##Compilação do Modelo\n",
        "\n",
        "Nessa parte do código, definimos os algoritmos de otimização, a função de perda e a métrica que será utilizada para avaliação do modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybvzy_1RgJHc"
      },
      "source": [
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss = losses.binary_crossentropy,\n",
        "              metrics = [metrics.binary_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtq3pVoMg1qK"
      },
      "source": [
        "##Configuração da Avaliação\n",
        "Reservaremos uma parte de nossos dados de treinamento para validação da precisão do modelo durante o treinamento. Um conjunto de validação nos permite monitorar o progresso de nosso modelo em dados não vistos anteriormente à medida que ele passa por épocas durante o treinamento.\n",
        "As etapas de validação nos ajudam a ajustar os parâmetros de treinamento da função `model.fit` para evitar overfitting e underfitting de dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPKKuXGeMBY_"
      },
      "source": [
        "# Input for Validation\n",
        "X_val = X_train[:10000]\n",
        "partial_X_train = X_train[10000:]\n",
        "\n",
        "# Labels for validation\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLWSviV6iEWR"
      },
      "source": [
        "##Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BqT9ntbMEPf"
      },
      "source": [
        "history = model.fit(partial_X_train,\n",
        "                   partial_y_train,\n",
        "                   epochs=20,\n",
        "                   batch_size=512,\n",
        "                   validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvY8J3x8h31X"
      },
      "source": [
        "##Visualizando os Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR2mgZS7MJ8S"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "\n",
        "# Plotting losses\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss_values, 'b', label=\"Validation Loss\")\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzeadSTcMUVe"
      },
      "source": [
        "### Retreinando o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68aYM4fNMU61"
      },
      "source": [
        "model.fit(partial_X_train,\n",
        "                   partial_y_train,\n",
        "                   epochs=3,\n",
        "                   batch_size=512,\n",
        "                   validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dpzFn4-Mbjv"
      },
      "source": [
        "# Making Predictions for testing data\n",
        "np.set_printoptions(suppress=True)\n",
        "result = model.predict(X_test)\n",
        "\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2iJqk86Mzlx"
      },
      "source": [
        "y_pred = np.zeros(len(result))\n",
        "for i, score in enumerate(result):\n",
        "    y_pred[i] = 1 if score > 0.5 else 0\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mae = mean_absolute_error(y_pred, y_test)\n",
        "mae"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}