{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03a_ClassificaçãoTextos-BoW-TF-IDF.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/mineracao_textual/blob/main/Classifica%C3%A7%C3%A3o/03a_Classifica%C3%A7%C3%A3oTextos_BoW_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x9LVwe-LiMi"
      },
      "source": [
        "# Aprendizado Bayesiano: Naive Bayes\n",
        "\n",
        "O algoritmo Naive Bayes, assim como o KNN é um algoritmo de aprendizado com implementação relativamente simples, e que pode levar a resultados muito bons em determinados problemas de classificação. Este consiste em aplicar o teorema de Bayes, com a prerrogativa de independencia condicional entre cada par de atributos dado o valor da classe.\n",
        "\n",
        "Clique nos links para mais informações sobre o algoritmo [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) e sobre [classificação de textos](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html). com scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMvflrmwLnJl"
      },
      "source": [
        "# Exemplo: classificação de texto\n",
        "\n",
        "Neste exemplo, utilizaremos um dataset chamado 20newsgroups. \n",
        "\n",
        "O conjunto de dados é uma coleção de aproximadamente 20.000 documentos de grupos de notícias, particionados (quase) uniformemente em 20 grupos/categorias de notícias diferentes. Mais informações sobre esta base podem ser obtidas no repositório [UCI](http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups)\n",
        "\n",
        "As categorias existentes são:\n",
        "\n",
        "* 'alt.atheism',\n",
        "* 'comp.graphics',\n",
        "* 'comp.os.ms-windows.misc',\n",
        "* 'comp.sys.ibm.pc.hardware',\n",
        "* 'comp.sys.mac.hardware',\n",
        "* 'comp.windows.x',\n",
        "* 'misc.forsale',\n",
        "* 'rec.autos',\n",
        "* 'rec.motorcycles',\n",
        "* 'rec.sport.baseball',\n",
        "* 'rec.sport.hockey',\n",
        "* 'sci.crypt',\n",
        "* 'sci.electronics',\n",
        "* 'sci.med',\n",
        "* 'sci.space',\n",
        "* 'soc.religion.christian',\n",
        "* 'talk.politics.guns',\n",
        "* 'talk.politics.mideast',\n",
        "* 'talk.politics.misc',\n",
        "* 'talk.religion.misc'\n",
        "\n",
        "Neste exemplo, vamos considerar apenas duas categorias: '**alt.med**' e '**comp.graphics**'. O scitkit contém uma função que auxilia o download desta base:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeOVilr9NIUG"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Categorias selecionadas\n",
        "categories = [\n",
        "    'comp.graphics',\n",
        "    'sci.med',\n",
        "]\n",
        "\n",
        "print(\"Carregando 20 newsgroups dataset para as categorias:\")\n",
        "print(categories)\n",
        "\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test  = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)\n",
        "print(\"%d documents\" % len(twenty_train.filenames))\n",
        "print(\"%d categories\" % len(twenty_train.target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X4g3K1-UlcW"
      },
      "source": [
        "# Visualizar os dados coletados\n",
        "print(twenty_train.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC-a8vReVU70"
      },
      "source": [
        "# Visualizando um dos documentos\n",
        "print(twenty_train['data'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKqq08LkczeD"
      },
      "source": [
        "# tamanho (sem preprocessamento)\n",
        "len(twenty_train['data'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1UM10zQPV_k"
      },
      "source": [
        "## Preprocessamento\n",
        "\n",
        "Como etapa de préprocessamento, iremos apenas nos limitar a remover palavras muito comuns, as chamadas *stopwords*. Uma lista de possíveis stopwords para inglês encontra-se abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOtv0Xd7Pj8m"
      },
      "source": [
        "stopwords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
        "             'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',  'can', \"can't\", 'cannot', 'could', \"couldn't\", \n",
        "             'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during','each', 'few', 'for', 'from', 'further', \n",
        "             'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", \n",
        "             'it', \"it's\", 'its', 'itself','1st', '2nd', '3rd','4th', '5th', '6th', '7th', '8th', '9th', '10th'\n",
        "             \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself','no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
        "             'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
        "             'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
        "             \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
        "             'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
        "             \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
        "             'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
        "             'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1ghOFTgKtzh"
      },
      "source": [
        "O código abaixo utiliza a biblioteca NLTK para fazer o pré-processamento dos textos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRtr11vNRMof"
      },
      "source": [
        "import string\n",
        "from nltk.tokenize.regexp import RegexpTokenizer\n",
        "\n",
        "def preprocess(text):\n",
        "  \n",
        "  # remover pontuações\n",
        "  text   = text.translate(string.punctuation)\n",
        "  \n",
        "  # converter para lowercase\n",
        "  text = text.lower()\n",
        "  \n",
        "  # tokenizar o texto em palavras\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(text.lower())\n",
        "\n",
        "  # filtrar palavras\n",
        "  tokens = [word for word in tokens\n",
        "            if word not in stopwords       # descartar stopwords\n",
        "                and len(word) > 3          # descartar palavras com menos de 3 caracteres\n",
        "                and not word[0].isdigit()] # descartar tokens contendo apenas numeros\n",
        "\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "X_train = []\n",
        "for doc in twenty_train['data']:\n",
        "  X_train.append(preprocess(doc))\n",
        "  \n",
        "X_test = []\n",
        "for doc in twenty_test['data']:\n",
        "  X_test.append(preprocess(doc))\n",
        "  \n",
        "print(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxcZX12qc5Sr"
      },
      "source": [
        "# tamanho (com preprocessamento)\n",
        "len(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aH2nkY3Sv1N"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Converter o a função de preprocessamento acima para utilizar o SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h02NOJ7US8Dc"
      },
      "source": [
        "# sua resposta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwUHXRvfMOc-"
      },
      "source": [
        "## Representação vetorial do texto\n",
        "\n",
        "Neste exemplo, utilizaremos o algoritmo Naive Bayes para classificar documentos de texto em categorias. Para isso, precisamos antes converter o texto para uma representação vetorial, ou seja, cada documento/exemplo precisa ser representado por um vetor de dimensões pré-definidas. Utilizaremos três técnicas básicas: BOW (*Bag of Words*), TF (*Term Frequency*) e TF-IDF (*Term Frequency - Inverse Document Frequency*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6o5yjjaPJCG"
      },
      "source": [
        "### BOW\n",
        "\n",
        "Consiste basicamente em contar quantas vezes cada palavra aparece no documento. Ou seja, sua aplicação a um conjunto de *n* documentos, produz uma matriz *n x d*, onde *d* corresponde ao tamanho do vocabulário considerado. No Scikit, esta representação é implementada pelo [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMmOZfv2SsZ4"
      },
      "source": [
        "# Exemplo de uso do BOW no scikit\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtimrxTfTs8T"
      },
      "source": [
        "Aplicando ao nosso dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkcoN3_XTu-I"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_model  = vectorizer.fit(X_train)\n",
        "\n",
        "X_bow_train = bow_model.transform(X_train)\n",
        "X_bow_test  = bow_model.transform(X_test)\n",
        "\n",
        "print(X_bow_train.shape,X_bow_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB-lp5d-Wej9"
      },
      "source": [
        "# matriz está armazenada em formato sparse\n",
        "print(X_bow_train[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6COLPTEVpJm"
      },
      "source": [
        "### *Term Frequency* (TF)\n",
        "A contagem de ocorrências (i.e., BOW) é um bom começo, mas há um problema: documentos mais longos terão valores de contagem média mais altos do que documentos mais curtos, embora possam falar sobre os mesmos tópicos.\n",
        "\n",
        "Para evitar essas possíveis discrepâncias, basta dividir o número de ocorrências de cada palavra em um documento pelo número total de palavras no documento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbJtPIvSVtke"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=False)\n",
        "tf_model = vectorizer.fit(X_train)\n",
        "\n",
        "X_tf_train = tf_model.transform(X_train)\n",
        "X_tf_test  = tf_model.transform(X_test)\n",
        "\n",
        "print(X_tf_train[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjQ0ITVjX8fV"
      },
      "source": [
        "### TF-IDF\n",
        "Palavras que aparecem em muitos documentos podem não ser interessantes para diferencia a classe desses documentos. Portanto, podemos dar um peso maior as palavras que aparecem em poucos documentos, pois essas palavras funcionam melhor para diferenciar as classes entre os documentos. TF-IDF é uma indicação do poder de discriminação do termo. O Log é usado para diminuir o efeito em relação a tf.\n",
        "\n",
        "Usando o scikit, basta ativar o flag `use_idf=True`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWUoJsm_YFG3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True)\n",
        "tfidf_model = vectorizer.fit(X_train)\n",
        "\n",
        "X_tfidf_train = tfidf_model.transform(X_train)\n",
        "X_tfidf_test  = tfidf_model.transform(X_test)\n",
        "\n",
        "print(X_tfidf_train[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTJq9ya-Yw0z"
      },
      "source": [
        "# Treinando modelo NaiveBayes (NB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYV00SwlbDhF"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW8Q3aN-axP_"
      },
      "source": [
        "## BOW + NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDOh6kBpbAN5"
      },
      "source": [
        "clf.fit(X_bow_train, twenty_train.target)\n",
        "\n",
        "acc = clf.score(X_bow_test , twenty_test.target)\n",
        "print('Acurácia: ', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PFI2MQ-a0_4"
      },
      "source": [
        "## TF + NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09QMVfd0buzP"
      },
      "source": [
        "clf.fit(X_tf_train, twenty_train.target)\n",
        "\n",
        "acc = clf.score(X_tf_test , twenty_test.target)\n",
        "print('Acurácia: ', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgfiTAx8a2zk"
      },
      "source": [
        "## TF-IDF + NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y24I7yk0Y2Wh"
      },
      "source": [
        "clf.fit(X_tfidf_train, twenty_train.target)\n",
        "\n",
        "acc = clf.score(X_tfidf_test , twenty_test.target)\n",
        "print('Acurácia: ', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzxDimJCaN-x"
      },
      "source": [
        "# Testando iterativamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BictP9Z9Z35L"
      },
      "source": [
        "docs = ['The patient is getting well', 'OpenGL on the GPU is fast']\n",
        "preprocessed_docs = [preprocess(doc) for doc in docs]\n",
        "\n",
        "print('\\nafter preprocessing:')\n",
        "print(preprocessed_docs)\n",
        "\n",
        "docs_preds = clf.predict(tfidf_model.transform(preprocessed_docs))\n",
        "\n",
        "print('\\npredictions:')\n",
        "for i,doc in enumerate(docs):\n",
        "    print('{} -> {}'.format(doc, twenty_train.target_names[docs_preds[i]]))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upoMExQA590q"
      },
      "source": [
        "## Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHy4D2kMaULb"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "preds = clf.predict(X_tfidf_test)\n",
        "\n",
        "\n",
        "#print(metrics.plot_confusion_matrix(clf, X_tfidf_test, twenty_test.target, \n",
        "#                                    display_labels=twenty_test.target_names,\n",
        "#                                    normalize='true'))\n",
        "\n",
        "\n",
        "print(metrics.classification_report(twenty_test.target, preds,\n",
        "                                    target_names=twenty_test.target_names))\n",
        "\n",
        "\n",
        "print('Matriz de Confusao:')\n",
        "cm = confusion_matrix(twenty_test.target,preds)\n",
        "f = sns.heatmap(cm, annot=True, fmt='d')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}