{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_NLP-Exercício-Spacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/mineracao_textual/blob/main/PLN/03_NLP_Exerc%C3%ADcio_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/valmirf/mineracao_textual.git\n"
      ],
      "metadata": {
        "id": "i-ffMssQQED2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLsDqQxqYTWi"
      },
      "source": [
        "# Pré-processamento de texto num problema real\n",
        "\n",
        "Começamos pelos imports:\n",
        "\n",
        "1.   Bibliotecas para manipulação de dados;\n",
        "2.   Spacy para processamento de texto;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeOVilr9NIUG"
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "\n",
        "#carrega biblioteca relacionada a spam em ingles\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfkRRYHjY9td"
      },
      "source": [
        "# Lendo os dados de uma planilha usando o pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X4g3K1-UlcW"
      },
      "source": [
        "df = pd.read_csv(\"mineracao_textual/Dados/spam.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_-yPkfkZAmr"
      },
      "source": [
        "# Pré-processamento\n",
        "\n",
        "Aqui aplicamos spacy a colina 'Text' do spam como vimos na aula. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC-a8vReVU70"
      },
      "source": [
        "# Print apenas na primeira iteração para saída não ficar muito grande \n",
        "contador = 0\n",
        "texto = df['Text']\n",
        "texto_processado = texto.copy()\n",
        "for texto in texto_processado:\n",
        "  doc = nlp(texto)\n",
        "  if contador == 0:\n",
        "      print(\"\\033[1m\\t{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\033[0m\".format(          \n",
        "          'Index',\n",
        "          'text',\n",
        "          'lemma',\n",
        "          'stop',\n",
        "          'punct',\n",
        "          'space',\n",
        "          'pos',\n",
        "          'dep',\n",
        "      ))  \n",
        "  for token in doc:\n",
        "    if contador == 0:         \n",
        "      print(\"\\t{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
        "            token.idx,\n",
        "            token.text,\n",
        "            token.lemma_,\n",
        "            token.is_stop,\n",
        "            token.is_punct,\n",
        "            token.is_space,\n",
        "            token.pos_,\n",
        "            token.dep_,\n",
        "      ))\n",
        "\n",
        "  contador +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28n3blcWZC95"
      },
      "source": [
        "##Remoção de pontuação\n",
        "\n",
        "Nessa etapa, remove a pontuação do texto e cria uma nova coluna com o texto processado sem pontuação. Note que no código abaixo, é utilizada uma estrutura mais avançada de programação em python com o uso do `lambda`. Você pode fazer a mesma coisa com o uso do comando `for`. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text_no_ponctuation_number'] = df['Text'].apply(lambda x: [token for token in x if token not in string.punctuation and not token.isnumeric()])\n",
        "df['Text_no_ponctuation_number'] = df['Text_no_ponctuation_number'].apply(lambda x: ''.join(x))\n"
      ],
      "metadata": {
        "id": "NaGzOGtYeDQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO673DrZZF9S"
      },
      "source": [
        "##Remoção de stopwords\n",
        "\n",
        "\n",
        "Nessa etapa, remove as stopwords. Nessa etapa, começamos a utilizar a biblioteca spacy com o comando `nlp(x)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKqq08LkczeD"
      },
      "source": [
        "df['Text_no_stopword'] = df['Text_no_ponctuation_number'].apply(lambda x: [token.text.lower() for token in nlp(x) if (token.is_stop == False and len(token.text)>3)])\n",
        "df['Text_no_stopword'] = df['Text_no_stopword'].apply(lambda x: ' '.join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E se usasse uma lista própria de stopwords? Como seria o processamento? (obs: use o código abaixo no exercício.)"
      ],
      "metadata": {
        "id": "0lCWfr7LfQ7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lista de stopwords\n",
        "stopwords = ['de','a ','o','que','e','do','da','em','um','para','é','com','não','uma','os','no','se','na','por','mais', \n",
        "            'as','dos','como','mas','foi','ao','ele','das','tem','à','seu','sua','ou','ser','quando','muito','há','nos','já','está','eu','também', \n",
        "            'só','pelo','pela','até','isso','ela','entre','era','depois','sem','mesmo','aos','ter','seus','quem','nas','me','esse','eles', \n",
        "            'estão','você','tinha','foram','essa','num','nem','suas','meu','às','minha','têm','numa','pelos','elas','havia','seja','qual',\n",
        "            'será','nós','tenho','lhe','deles','essas','esses','pelas','este','fosse','dele','tu','te','vocês','vos', \n",
        "            'lhes','meus','minhas','teu','tua','teus','tuas','nosso','nossa','nossos','nossas','dela','delas','esta','estes','estas','aquele', \n",
        "            'aquela','aqueles','aquelas','isto','aquilo','estou','está','estamos','estão','estive','esteve','estivemos','estiveram','estava',\n",
        "            'estávamos','estavam','estivera','estivéramos','esteja','estejamos','estejam','estivesse','estivéssemos','estivessem','estiver',\n",
        "            'estivermos','estiverem','hei','há','havemos','hão','houve','houvemos','houveram','houvera','houvéramos','haja','hajamos'\n",
        "            'hajam','houvesse','houvéssemos','houvessem','houver','houvermos','houverem','houverei','houverá','houveremos','houverão','houveria',\n",
        "            'houveríamos','houveriam','sou','somos','são','era','éramos','eram','fui','foi','fomos','foram','fora','fôramos','seja','sejamos',\n",
        "            'sejam','fosse','fôssemos','fossem','for','formos','forem','serei','será','seremos','serão','seria','seríamos','seriam','tenho',\n",
        "            'tem','temos','tém','tinha','tínhamos','tinham','tive','teve','tivemos','tiveram','tivera','tivéramos','tenha','tenhamos','tenham',\n",
        "            'tivesse','tivéssemos','tivessem','tiver','tivermos','tiverem','terei','terá','teremos','terão','teria','teríamos','teriam']"
      ],
      "metadata": {
        "id": "tZnZ-GNKfYfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NASi7A7UZJqG"
      },
      "source": [
        "##Lematização e remoção de stopwords\n",
        "\n",
        "Essa etapa demonstra a etapa de lematização no texto sem stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxCD38dwQ10g"
      },
      "source": [
        "df['Text_lemma_no_stopword'] = df['Text_no_stopword'].apply(lambda x: [token.lemma_ for token in nlp(x)])\n",
        "df['Text_lemma_no_stopword'] = df['Text_lemma_no_stopword'].apply(lambda x: ' '.join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEngG5j_ZNEN"
      },
      "source": [
        "Lematização no texto sem a retirada de stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7A2BmjxRLiD"
      },
      "source": [
        "df['Text_lemma'] = df['Text_no_ponctuation_number'].apply(lambda x: [token.lemma_ for token in nlp(x)])\n",
        "df['Text_lemma'] = df['Text_lemma'].apply(lambda x: ' '.join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIw4Wb4kbv_H"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercício\n",
        "\n",
        "1 - Repita todo pré-processamento acima na base de dados de petições pra reclamações de passageiros em casos de viagens. Essa base de dados pode ser encontrada no github da disciplina. Ela possui algumas características diferentes, a língua e as colunas são diferentes, por exemplo. Você deve adequar o código acima pra que leia e faça o pré-processamento corretamente para a base de dados de petições. \n",
        "\n",
        "Comando para carregar a base de dados do exercício:\n",
        "\n",
        "`df = pd.read_csv(\"mineracao_textual/Dados/dataset_ness_law.csv\")`"
      ],
      "metadata": {
        "id": "m93sU-fiiT66"
      }
    }
  ]
}