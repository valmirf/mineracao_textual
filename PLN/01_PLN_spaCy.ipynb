{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "01-PLN-spaCy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/mineracao_textual/blob/main/PLN/01_PLN_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CrK2e4Skshf"
      },
      "source": [
        "# spaCy\n",
        "\n",
        "A bliblioteca spaCy se tornou bastante popular nos últimos anos para tarefas de PLN. Ela é mais recente que o NLTK, projetada especificamente para i) trabalhar em problemas maiores e ii) ocultar detalhes irrelevantes para o  usuário. Vamos nos concentrar nos principais recursos:\n",
        "\n",
        "[Carregando modelos pré-treinados](#loading)<br>\n",
        "\n",
        "[Tokenization](#tokenization)<br>\n",
        "\n",
        "[Lemmatization](#lemma)<br>\n",
        "\n",
        "[Named entity recognition (NER)](#ner)<br>\n",
        "\n",
        "[Vizualizando NER](#visualize-ner)<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rndqN2HxleIw"
      },
      "source": [
        "# Instalação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U_RUJ4ZlhGC"
      },
      "source": [
        "#instalando bibliotecas\n",
        "!pip install spacy\n",
        "!spacy download pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09htAjC3kshg"
      },
      "source": [
        "import os\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUa76gWJkshk"
      },
      "source": [
        "## Loading spaCy models <a id='loading'></a>\n",
        "\n",
        "O spaCy disponibiliza uma série de [modelos](https://spacy.io/usage/models) pré-treinados, que devem ser baixados e podem funcionar em problemas mais genéricos e possui modelos diferentes para diferentes idiomas, inclusive em português. \n",
        "\n",
        "Para fazer uso dos modelos, primeiro os carregamos no spaCy com \n",
        "\n",
        "`nlp = spacy.load ('en')`, que armazena o modelo em uma variável chamada nlp para nós. O `'en'` significa inglês. Se você deseja processar dados em outros idiomas, primeiro precisa baixar os modelos relevantes e carregá-los de maneira semelhante.\n",
        "\n",
        "O modelo aqui referenciado para o português `pt` corresponde a um modelo de CNN treinado em um subconjunto da base WikiNER. Além do `pt_core_news_sm`, o spacy possui o `pt_core_news_md` e o `pt_core_news_lg`. Detalhes das bases utilizadas e acurácia de cada modelo podem ser vistos [aqui](https://spacy.io/models/pt). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYc5VJ3Ikshl"
      },
      "source": [
        "nlp = spacy.load('pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PheBKMAgksho"
      },
      "source": [
        "O modelo agora pode ser acessado através da variável `nlp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRPytSihn9Ik"
      },
      "source": [
        "doc = nlp(\"Eu gostaria que as aboboras viessem com mais sementes.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZXiXd4ukshu"
      },
      "source": [
        "## Tokenization <a id='tokenization'></a>\n",
        "\n",
        "A tokenização no spaCy é bem simples de ser utilizada com os modelos pré-treinados. Quando iteramos sobre um objeto `Doc`, spaCy assume que queremos iterar sobre os tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uERG0k6Ukshv"
      },
      "source": [
        "for token in doc:\n",
        "    print(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ndAAQ6lksh7"
      },
      "source": [
        "# quantidade de tokens\n",
        "len(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXZzON8rkshy"
      },
      "source": [
        "Cada `token` em `doc` é uma instância da classe `Token`. Este objeto armazena uma série de informações relevantes como a representação em string do token (`.text`), o índice (`.idx`) e ainda aspectos linguísticos como `.is_stop`,  `.is_space`, `.lemma_`  e `.pos_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCKCNrc2kshy"
      },
      "source": [
        "print(\"\\033[1m\\t{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\033[0m\".format(\n",
        "        'Index',\n",
        "        'lemma',\n",
        "        'stop',\n",
        "        'punct',\n",
        "        'space',\n",
        "        'pos',\n",
        "        'dep',\n",
        "    ))\n",
        "\n",
        "for token in doc:    \n",
        "    print(\"\\t{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\".format(\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_stop,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.pos_,\n",
        "        token.dep_,\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0llhdANksh-"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Verifique a quantidade de tokens considerados stopwords e pontuação do texto abaixo. Imprima uma versão do texto removendo este tipo de tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Cvtyplksh-"
      },
      "source": [
        "# Texto\n",
        "review = '''Este é sem dúvida o pior filme que eu já vi. E acredite em mim, eu vi muitos filmes. A reviravolta inacreditável que o filme faz - passando de um extremamente mau filme \"Formas de vida alienígenas habitam a terra\", com um filme que tenta espalhar um arquicristiano \"O dia do julgamento está próximo, buscar Jesus ou queimar por toda a eternidade em as dívidas ardentes do inferno \"mensagem - deixou-me atordoado depois de ter sido atormentado por 85 minutos. Até mesmo os cristãos religiosos devem se envergonhar ou ficar furiosos ao ver suas crenças postadas dessa maneira. Eu não sabia o que fazer comigo quando assisti a atuação horrível que poderia ter sido realizada por crianças de 7 anos de idade. Simplesmente repugnante. Eu não sou cristão nem muito religioso. Mas se eu estivesse, não teria mais medo do Inferno. Rich Christiano mostrou ser algo muito pior.'''\n",
        "\n",
        "# sua resposta\n",
        "doc = nlp(review)\n",
        "counter = 0\n",
        "words = []\n",
        "stopword = []\n",
        "for token in doc:\n",
        "  if token.is_stop or token.is_punct:\n",
        "    stopword.append(token)\n",
        "    counter+=1\n",
        "  else:\n",
        "    words.append(token)\n",
        "print('counter', counter)\n",
        "print(words)\n",
        "print(stopword)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkNV8lO_qHpp"
      },
      "source": [
        "# Detecção de sentenças\n",
        "\n",
        "As sentenças detectadas pelo modelo ficam armazenadas no atributo `.sents` do objeto `Doc`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZf4oy_UqKva"
      },
      "source": [
        "doc = nlp(\"Eu estarei em Recife próxima semana. Será que levo roupa de frio?\")\n",
        "count = 1\n",
        "for sent in doc.sents:\n",
        "    print('Sentença ' + str(count) + ': ')\n",
        "    print(sent)\n",
        "    count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBlENetrrEX0"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "1. Quantas sentenças existem no texto armazenado na variável `review`?\n",
        "\n",
        "2. Quais são essas sentencas?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0jZIC-TqY8r"
      },
      "source": [
        "# sua resposta\n",
        "doc = nlp(review)\n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0RoWaxJksiC"
      },
      "source": [
        "## POS tagging\n",
        "\n",
        "Como vimos acima, ao submeter uma string ao modelo, o spacy aplica todo o pipeline de PLN ao texto, incluindo o processo de POS Tagging:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeV9D0KHksiC"
      },
      "source": [
        "doc = nlp(\"Eu estarei em Recife próxima semana. Será que levo roupa para frio?\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joSrl8nMksiE"
      },
      "source": [
        "Esses rótulos podem ser um pouco difíceis de interpretar... O que significa `PROPN`? e `ADP`? Use a função `explain` para obter as respostas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTcRmTMbksiF"
      },
      "source": [
        "print(spacy.explain('PROPN'))\n",
        "print(spacy.explain('ADP'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfEU0kS9ksiH"
      },
      "source": [
        "Observe que o texto (label) é sempre armazenado em um atributo com `_` no fim, pois o spaCy armazena internamente tudo na forma de hashes, para tornar o código mais eficiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dNcZc6jksiI"
      },
      "source": [
        "for token in doc[:10]:\n",
        "    print(token.text, token.pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Y2TpLyksiL"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Retorne a lista de todas as POS Tags da variável `review` como uma lista de tuplas (word, pos) para cada token no texto. Quais as tags mais frequentes?\n",
        "\n",
        "Dica: você pode utilizar um `pandas.DataFrame` para facilitar as contagens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRlH28-wksiL"
      },
      "source": [
        "# sua resposta\n",
        "import pandas as pd\n",
        "\n",
        "doc = nlp(review)\n",
        "words = []\n",
        "postags = []\n",
        "for token in doc:\n",
        "  words.append(token.text)\n",
        "  postags.append(token.pos_)\n",
        "\n",
        "df = pd.DataFrame(data={'words': words, 'pos':postags})\n",
        "df.words.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngAxxubYxGar"
      },
      "source": [
        "df.pos.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlLNgH7GksiN"
      },
      "source": [
        "## Lemmatization <a id='lemma'></a>\n",
        "\n",
        "O spaCy não disponibiliza muitos detalhes de escolha de algoritmos para Lemmatization, o que funciona bem para a maioria dos casos, diferentemente do NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZkVi_sMksiO"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HqUBmnnksiR"
      },
      "source": [
        "## Named entity recognition (NER) <a id='ner'></a>\n",
        "\n",
        "Named entity recognition (NER) é uma das principais tarefas em projetos de recuperação e extração de informação em textos. Muitas tarefas se iniciam a partir da detecção de entidades nomeadas, como a extração de relações por exemplo. \n",
        "\n",
        "Em NER, as diferentes entidades nomeadas extraídas são agrupadas por tipo. Por exemplo, \"pessoa\", \"organização\", \"local\", \"país\" etc. No spaCy, existem muitos [tipos diferentes](https://spacy.io/api/annotation#named-entities) de entidades nomeadas que ele pode extrair com modelos pré-treinados.\n",
        "\n",
        "As entidades nomeadas no spaCy estão disponíveis como propriedade `ents` de um` Doc`. O `.label_` nos diz o tipo de entidade nomeada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIQD7vOTksiS"
      },
      "source": [
        "doc = nlp(\"Eu estarei em Recife próxima semana. Será que levo roupa para frio?\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text,ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFMZNuh-ksiV"
      },
      "source": [
        "doc = nlp(\"Geraldo Júlio é o prefeito de Recife.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text,ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X05yeFFoDiVE"
      },
      "source": [
        "#explain também funciona para entidades\n",
        "spacy.explain('LOC')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNAqQh_oksiX"
      },
      "source": [
        "### Exercício 1\n",
        "\n",
        "Extraia todas as entidades nomeadas da variável `review`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOR6zb-YksiY"
      },
      "source": [
        "doc = nlp(review)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text,' - ',ent.label_,' - ',spacy.explain(ent.label_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq3jc_2p1avB"
      },
      "source": [
        "### Exercício 2\n",
        "Utilizar a lib do python `wikipedia`, para baixar o conteúdo da página referente ao cantor Luiz Gonzaga no wikipedia em português, e analise quais as entidades presentes nas 10 primeiras sentenças do texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP_uLoVJksia"
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErHz8BJ-ksic"
      },
      "source": [
        "import wikipedia\n",
        "wikipedia.set_lang(\"pt\")\n",
        "p = wikipedia.page(\"Luiz Gonzaga\")\n",
        "\n",
        "print(p.url)\n",
        "print(p.title)\n",
        "\n",
        "content = p.content "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdZ_Bg9q3Hi6"
      },
      "source": [
        "# sua resposta\n",
        "doc = nlp(content)\n",
        "for sent in list(doc.sents)[0:10]:\n",
        "  for ent in sent.ents:\n",
        "        print(ent.text,' - ',ent.label_,' - ',spacy.explain(ent.label_))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43ZXWVoAksie"
      },
      "source": [
        "## Visualização NER <a id='visualize-ner'></a>\n",
        "\n",
        "O displaCy é uma extensão do spaCy para visualização do processo de PLN. \n",
        "\n",
        "Após importar a lib `displacy`, podemos usar o método `render` sobre o `doc` criado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGCp3Pzfksif"
      },
      "source": [
        "from spacy import displacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orXlFlfjksii"
      },
      "source": [
        "doc = nlp(review)\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GOU_UyY4Srt"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Utilize o `displacy` para visualizar as entidades encontradas em algumas das sentenças do conteúdo da página do wikipedia analisada no exercício anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJhvxC_04kUE"
      },
      "source": [
        "# sua resposta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyBBECKoksik"
      },
      "source": [
        "## Parsing de Dependencia\n",
        "\n",
        "A análise de dependência refere-se a desenhar os relacionamentos entre palavras individuais em uma frase. Assim como o NER, esse é um tópico bem importante em PLN. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SVmTfMFksil"
      },
      "source": [
        "for token in doc[:18]:\n",
        "    print(token.text, token.dep_, token.head,' - ', spacy.explain(token.dep_))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpimXdJDksio"
      },
      "source": [
        "Utilizando o `displacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwKtQQbUksip"
      },
      "source": [
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cld4BFg_54E8"
      },
      "source": [
        "# modificando a visualização\n",
        "displacy.render(doc, style='dep', jupyter=True, options={\"compact\": True})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN6wn9tnksir"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Crie uma lista para cada tipo de entidade no texto da wikipedia analisado, ou seja: \n",
        "- texto (string)\n",
        "- pos\n",
        "- lemma\n",
        "- se é uma stopword (`.is_stop`)\n",
        "- se é pontuação (`.is_punct`)\n",
        "- se é um número (`.like_num`)\n",
        "- a relação de dependência (`.dep_`)\n",
        "\n",
        "Utilize as listas criadas para criar um `pandas.DataFrame`, e analise a distribuição das categorias registradas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65kvgT1eksir"
      },
      "source": [
        "doc = nlp(content)\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "# resposta\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}